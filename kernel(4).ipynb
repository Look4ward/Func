{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling\nimport pandas_summary\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport gc\nimport time\nimport psutil\nimport random as rn\n\n\nfrom tqdm import tqdm\nfrom contextlib import contextmanager\nfrom tensorflow import set_random_seed\n\nrn.seed(5)\nnp.random.seed(7)\nset_random_seed(2)\nos.environ['PYTHONHASHSEED'] = '3'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4d7c190c7dcda954395298be496a0c63a6c8b49a"
      },
      "cell_type": "markdown",
      "source": "# Functions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e331bc1133f01b3a5e44be94687ed30f03345c67",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def data_desc(df):\n    print (df.info())\n    print (df.describe())\n    print (df.columns)\n    x_rowcount = df.shape[0] #be careful not to use df1.count() which returns only non-NaN values\n    print ('There are ', x_rowcount, \"rows in the file\\n\")\n    #print (df.isnull())\n    #print (\"\\n\",df.dtypes)\n    print (\"\\n\",df.head(),\"\\n\",df.tail())\n    df.dtypes.value_counts()\n    return\n\ndef data_target(df,target):\n    print (df[target].value_counts())\n    return\n\n\ndef data_cat_val (df):\n    for col in df:\n        if df[col].dtype == 'object':\n            df[col].fillna('missing')\n    print (df.select_dtypes(include = ['object']).apply(pd.Series.nunique, axis =0))\n    return df\n\n\ndef data_corr (df,target):\n    # Find correlations with the target and sort\n    correlations = df.corr()[target].sort_values()\n\n    # Display correlations\n    print('Most Positive Correlations:\\n', correlations.tail(15))\n    print('\\nMost Negative Correlations:\\n', correlations.head(15))\n    return correlations\n\n# Function to calculate missing values by column \ndef data_mis_val(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table = mis_val_table[mis_val_table.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table\n\ndef data_encoder(df):\n    # Create a label encoder object\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in df:\n        if df[col].dtype == 'object':\n            # If 2 or fewer unique categories\n            if len(list(df[col].unique())) <= 2:\n                # Train on the training data\n                le.fit(df[col])\n                # print (col)\n                # Transform both training and testing data\n                df[col] = le.transform(df[col])\n                # Keep track of how many columns were label encoded\n                le_count += 1\n    print('%d columns were label encoded.' % le_count)\n    return\n\ndef data_dummy(df):\n    df = pd.get_dummies(df)\n    print (df.tail())\n    print('Training Features shape: ', df.shape)\n    return\n\ndef data_align (df1,target, df2):\n    train_labels = df1[target]\n    # Align the training and testing data, keep only columns present in both dataframes\n    df1, df2 = df1.align(df2, join = 'inner', axis = 1)\n\n    # Add the target back in\n    df1[target] = train_labels\n\n    print('Training Features shape: ', df1.shape)\n    print('Testing Features shape: ', df2.shape)\n    return\n\ndef plot_kde(df,target,feature):\n    plt.figure(figsize = (6, 4))\n\n    # KDE plot of target == 0\n    sns.kdeplot(df.loc[df[target] == 0, feature], label = 'target == 0')\n\n    # KDE plot of target == 1\n    sns.kdeplot(df.loc[df[target] == 1, feature], label = 'target == 1')\n\n    # Labeling of plot\n    plt.xlabel(feature); plt.ylabel('Density'); plt.title('Distribution of '+ feature);\n    \n    return\n\n\ndef plot_hist (df,target):\n\n    # Set the style of plots\n    plt.style.use('fivethirtyeight')\n    \n    # Plot the distribution    \n    plt.hist(df[target], edgecolor = 'k', bins = 20)\n    plt.title(target); plt.xlabel('Values'); plt.ylabel('Count');\n    return\n\ndef cal_woe(df, target):\n    num_events = target.sum()\n    num_non_events = target.shape[0] - target.sum()\n\n    feature_list = []\n    feature_iv_list = []\n    for col in df.columns:\n        if df[col].unique().shape[0] == 1:\n            del df[col]\n            print('remove constant col', col)\n\n        with timer('cope with %s' % col):\n            feature_list.append(col)\n\n            woe_df = pd.DataFrame()\n            woe_df[col] = df[col]\n            woe_df['target'] = target\n            events_df = woe_df.groupby(col)['target'].sum().reset_index().rename(columns={'target' : 'events'})\n            events_df['non_events'] = woe_df.groupby(col).count().reset_index()['target'] - events_df['events']\n            def cal_woe(x):\n                return np.log( ((x['non_events']+0.5)/num_non_events) / ((x['events']+0.5)/num_events)  )\n            events_df['WOE_'+col] = events_df.apply(cal_woe, axis=1)\n\n            def cal_iv(x):\n                return x['WOE_'+col]*(x['non_events'] / num_non_events - x['events'] / num_events)\n            events_df['IV_'+col] = events_df.apply(cal_iv, axis=1)\n\n            feature_iv = events_df['IV_'+col].sum()\n            feature_iv_list.append(feature_iv)\n\n            events_df = events_df.drop(['events', 'non_events', 'IV_'+col], axis=1)\n            df = df.merge(events_df, how='left', on=col)\n    iv_df = pd.DataFrame()\n    iv_df['feature'] = feature_list\n    iv_df['IV'] = feature_iv_list\n    iv_df = iv_df.sort_values(by='IV', ascending=False)\n    return df, iv_df\n\ntimer_depth = -1\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    global timer_depth\n    timer_depth += 1\n    yield\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memoryUse = py.memory_info()[0] / 2. ** 30\n    print('----'*timer_depth + f'>>[{name}] done in {time.time() - t0:.0f} s ---> memory used: {memoryUse:.4f} GB', '')\n    if(timer_depth == 0):\n        print('\\n')\n    timer_depth -= 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read Data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c729302814f42893799b16701ac07bc431160ec8"
      },
      "cell_type": "code",
      "source": "# Training data\ndf1 = pd.read_csv('../input/application_train.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a79814e6ddfe2a9f5c0e0c7a812c4521ec6f574b",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_desc(df1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5253ee2855e4d3afe8a8265129000e713cc635b9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pandas_summary(df1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5442fc675681aec0b38b9cb671a109fc23cc36b3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df1.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c0e3405aec10c5539c98e7913c5931db467ee03",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_target(df1,'TARGET')\n\nplot_hist(df1,'TARGET')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9725ee5f55190d90824465d3893f15cfd701d9bd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_mis_val(df1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f41077c69c911746053c00ee39b09acc4a02457d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df1 = data_cat_val(df1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ccdf29510efa1469d6bee90d1de1e0184183f7f3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df1_target = df1.pop('TARGET')\nwith timer('calculate WOE and IV'):\n    df1, iv_df = cal_woe(df1, df1_target)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57545502b2f759ba95129319930c04515fd1e777",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df1.tail(100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "99eb3a645d7f8a1978e6edad5312678f3bc83940"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}